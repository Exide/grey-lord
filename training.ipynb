{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8832c08c-4ec1-4583-97ee-26743ef2fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary contains 262 items.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_file(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "\n",
    "vocab_to_int = load_file(\"vocab_to_int.json\")\n",
    "int_to_vocab = load_file(\"int_to_vocab.json\")\n",
    "\n",
    "print(f\"Loaded vocabulary contains {len(vocab_to_int)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ebe4e9b-2187-47ad-99fe-3e6fe24ba13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_delay(token_str):\n",
    "    delay_value_str = token_str.split('#')[1].strip('|>')\n",
    "    delay_value = float(delay_value_str)\n",
    "    \n",
    "    # Apply our binning rules to pick the final token\n",
    "    if delay_value <= 1.0:\n",
    "        final_token = '<|delay_short|>'\n",
    "    elif delay_value <= 5.0:\n",
    "        final_token = '<|delay_medium|>'\n",
    "    else:\n",
    "        final_token = '<|delay_long|>'\n",
    "    \n",
    "    return final_token\n",
    "\n",
    "def custom_tokenizer(byte_stream: bytes, vocab: dict) -> list[int]:\n",
    "    token_ids = []\n",
    "\n",
    "    # This regex pattern is the core of the parser. It finds one of two things:\n",
    "    # 1. (<\\|[^|>]+?\\|>): A full special token, like '<|client|>' or '<|delay#1.23|>'.\n",
    "    # 2. (.): Any single character (byte) that is NOT part of a special token.\n",
    "    # The rb'' prefix means the pattern is a raw *byte* pattern.\n",
    "    pattern = re.compile(rb'(<\\|[^|>]+?\\|>)|(.)')\n",
    "\n",
    "    for match in pattern.finditer(byte_stream):\n",
    "        special_token_bytes, byte_char = match.groups()\n",
    "\n",
    "        if special_token_bytes:\n",
    "            token_str = special_token_bytes.decode('utf-8')\n",
    "\n",
    "            if token_str.startswith('<|delay#'):\n",
    "                token_id = vocab[tokenize_delay(token_str)]\n",
    "            else:\n",
    "                token_id = vocab.get(token_str)\n",
    "            \n",
    "            token_ids.append(token_id)\n",
    "\n",
    "        elif byte_char:\n",
    "            byte_value = int.from_bytes(byte_char, 'big')\n",
    "            token_ids.append(byte_value)\n",
    "\n",
    "\n",
    "    return token_ids\n",
    "\n",
    "# test the tokenizer\n",
    "sample_data = b'<|client|><|delay#0.85|>\\x48\\x65\\x6c\\x6c\\x6f<|server|><|delay#9.9|>\\x57\\x6f\\x72\\x6c\\x64'\n",
    "tokenized_output = custom_tokenizer(sample_data, vocab_to_int)\n",
    "actual = len(tokenized_output)\n",
    "expected = 14\n",
    "assert actual == expected, f'Expected {expected}, Actual {actual}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f129f993-c54f-4422-84a5-40fbab79bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ByteStreamDataset(Dataset):\n",
    "    def __init__(self, paths, vocab):\n",
    "        self.file_paths = paths\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_byte_stream = f.read()\n",
    "\n",
    "        token_ids = custom_tokenizer(raw_byte_stream, self.vocab)\n",
    "\n",
    "        return torch.tensor(token_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bd51ed-8783-4e66-86fb-2c2b903dff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch, pad_value):\n",
    "    max_len = max(len(seq) for seq in batch)\n",
    "\n",
    "    input_ids = torch.full((len(batch), max_len), pad_value, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((len(batch), max_len), dtype=torch.long)\n",
    "    \n",
    "    for i, seq in enumerate(batch):\n",
    "        input_ids[i, :len(seq)] = seq\n",
    "        attention_mask[i, :len(seq)] = 1\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8797826b-6e2a-47a6-969d-9b53c1a5bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens):\n",
    "    return [int_to_vocab.get(str(t.item()), f\"byte_{t.item()}\") for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06431091-e9e7-459e-b6a0-7d075abcd246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a DataLoader with 72 samples and a batch size of 1.\n",
      "pulled a batch from the DataLoader\n",
      "\n",
      "--- Checking Batch Structure ---\n",
      "âœ… [PASS] Batch is a dictionary.\n",
      "âœ… [PASS] Batch has the correct keys: ['input_ids', 'attention_mask']\n",
      "\n",
      "--- Checking Tensor Shapes ---\n",
      "âœ… [PASS] Tensors have matching shapes: torch.Size([1, 75657])\n",
      "   (Batch Size, Max Sequence Length in this Batch)\n",
      "\n",
      "--- Checking Token ID Value Range ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 4. Check the input_ids value range (THE MOST IMPORTANT CHECK)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Checking Token ID Value Range ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m vocab_size = model.config.vocab_size\n\u001b[32m     48\u001b[39m min_id = input_ids.min()\n\u001b[32m     49\u001b[39m max_id = input_ids.max()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(r'C:\\Users\\exide\\code\\c-telnet-proxy')\n",
    "file_pattern = r'*T*_*-*-*-*-*'\n",
    "file_paths = sorted(data_dir.glob(file_pattern))\n",
    "if not file_paths: raise FileNotFoundError\n",
    "\n",
    "dataset = ByteStreamDataset(paths=file_paths, vocab=vocab_to_int)\n",
    "pad_token_id = vocab_to_int['<|pad|>']\n",
    "\n",
    "batch_size = 1\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: collate_fn(batch, pad_token_id))\n",
    "\n",
    "print(f\"Created a DataLoader with {len(dataset)} samples and a batch size of {batch_size}.\")\n",
    "\n",
    "try:\n",
    "    # 'next(iter(data_loader))' gets the first batch\n",
    "    batch = next(iter(data_loader))\n",
    "    print(\"pulled a batch from the DataLoader\")\n",
    "except Exception as e:\n",
    "    print(f\"could not pull a batch: {e}\")\n",
    "    raise e\n",
    "\n",
    "# 2. Check the batch structure (type and keys)\n",
    "print(\"\\n--- Checking Batch Structure ---\")\n",
    "assert isinstance(batch, dict), f\"Batch should be a dict, but got {type(batch)}\"\n",
    "print(\"âœ… [PASS] Batch is a dictionary.\")\n",
    "\n",
    "expected_keys = {'input_ids', 'attention_mask'}\n",
    "assert set(batch.keys()) == expected_keys, f\"Expected keys {expected_keys}, but got {set(batch.keys())}\"\n",
    "print(f\"âœ… [PASS] Batch has the correct keys: {list(batch.keys())}\")\n",
    "\n",
    "# 3. Check the tensor shapes\n",
    "print(\"\\n--- Checking Tensor Shapes ---\")\n",
    "input_ids = batch['input_ids']\n",
    "attention_mask = batch['attention_mask']\n",
    "\n",
    "assert input_ids.shape == attention_mask.shape, \"Shapes of input_ids and attention_mask must match!\"\n",
    "print(f\"âœ… [PASS] Tensors have matching shapes: {input_ids.shape}\")\n",
    "print(f\"   (Batch Size, Max Sequence Length in this Batch)\")\n",
    "\n",
    "# 4. Check the input_ids value range (THE MOST IMPORTANT CHECK)\n",
    "print(\"\\n--- Checking Token ID Value Range ---\")\n",
    "vocab_size = model.config.vocab_size\n",
    "min_id = input_ids.min()\n",
    "max_id = input_ids.max()\n",
    "\n",
    "print(f\"   Model vocabulary size: {vocab_size}\")\n",
    "print(f\"   Minimum ID in batch: {min_id}\")\n",
    "print(f\"   Maximum ID in batch: {max_id}\")\n",
    "\n",
    "assert min_id >= 0, f\"Found a negative token ID: {min_id}\"\n",
    "assert max_id < vocab_size, f\"FATAL: Max ID ({max_id}) is out of bounds for vocab size ({vocab_size})!\"\n",
    "print(\"âœ… [PASS] All token IDs are within the valid range [0, vocab_size - 1].\")\n",
    "\n",
    "# 5. Check the attention mask correspondence\n",
    "print(\"\\n--- Checking Attention Mask ---\")\n",
    "# We'll check the last sequence in the batch as an example\n",
    "last_sequence_ids = input_ids[-1]\n",
    "last_sequence_mask = attention_mask[-1]\n",
    "\n",
    "# Find where the padding begins in this sample\n",
    "# (This requires you to know your pad_token_id)\n",
    "pad_indices = (last_sequence_ids == pad_token_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "if len(pad_indices) > 0:\n",
    "    first_pad_index = pad_indices[0].item()\n",
    "    print(f\"   Sample has padding, which starts at index: {first_pad_index}\")\n",
    "    # The token before padding should have a mask of 1\n",
    "    if first_pad_index > 0:\n",
    "        assert last_sequence_mask[first_pad_index - 1] == 1, \"Mask should be 1 right before padding starts.\"\n",
    "    # The first padding token should have a mask of 0\n",
    "    assert last_sequence_mask[first_pad_index] == 0, \"Mask should be 0 at the first padding token.\"\n",
    "    print(\"âœ… [PASS] Attention mask correctly marks padding with 0s (checked one sample).\")\n",
    "else:\n",
    "    # If there's no padding, the mask should be all 1s\n",
    "    assert last_sequence_mask.all(), \"If there is no padding, mask should be all 1s.\"\n",
    "    print(\"âœ… [PASS] Sample has no padding and mask is all 1s, which is correct.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nðŸŽ‰ --- Sanity Check Complete: DataLoader appears to be in a good state! --- ðŸŽ‰\")\n",
    "\n",
    "# tests for the input tensor\n",
    "input_ids = batch['input_ids']\n",
    "\n",
    "print(f\"\\nShape of the batch tensor: {input_ids.shape}\")\n",
    "print(input_ids)\n",
    "\n",
    "sequence_id = 0\n",
    "first_sequence = input_ids[sequence_id]\n",
    "\n",
    "first_50_tokens = first_sequence[:50]\n",
    "print(f\"\\nSample {sequence_id} start: {decode(first_50_tokens)}...\")\n",
    "\n",
    "last_50_tokens = first_sequence[-50:]\n",
    "print(f\"\\nSample {sequence_id} end: ...{decode(last_50_tokens)}\")\n",
    "\n",
    "# tests for the attention mask\n",
    "attention_mask = batch['attention_mask']\n",
    "\n",
    "print(f\"\\nShape of the attention_mask tensor: {attention_mask.shape}\")\n",
    "print(attention_mask[0, -50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "656881da-a8a1-4fd1-8f2a-e9fe27a30efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 262\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, AutoModelForCausalLM\n",
    "\n",
    "vocab_size = len(int_to_vocab)\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=1024,   # The maximum sequence length the model can handle\n",
    "    n_embd=256,         # The embedding dimension (vector size for each token)\n",
    "    n_layer=6,          # The number of Transformer layers\n",
    "    n_head=8            # The number of attention heads\n",
    ")\n",
    "\n",
    "def Transformer():\n",
    "    return AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "332c1929-87e4-4e0c-a426-6dbdc12b4263",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m5e-5\u001b[39m)\n\u001b[32m      4\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model.to(device)\n\u001b[32m      7\u001b[39m num_epochs = \u001b[32m3\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\modeling_utils.py:3698\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3693\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3694\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3697\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().to(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         module._apply(fn)\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         module._apply(fn)\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m     param_applied = fn(param)\n\u001b[32m    928\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1320\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1321\u001b[39m             device,\n\u001b[32m   1322\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1323\u001b[39m             non_blocking,\n\u001b[32m   1324\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1325\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1327\u001b[39m         device,\n\u001b[32m   1328\u001b[39m         dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1329\u001b[39m         non_blocking,\n\u001b[32m   1330\u001b[39m     )\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = Transformer() \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "print(f\"running {num_epochs} iterations\")\n",
    "\n",
    "model.train()\n",
    "print(\"model in training mode\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"training epoch {epoch}\")\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Move data to the GPU/CPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # ---> The Key Idea for Training <---\n",
    "        # The model's input is the sequence.\n",
    "        # The 'labels' are the same sequence, shifted one to the left.\n",
    "        # The model tries to predict token[i+1] using all tokens up to i.\n",
    "        # The model's loss function handles this shifting internally.\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # --- DEBUGGING STEP ---\n",
    "        # Check the min and max values in the batch *before* calling the model.\n",
    "        # The max value should be less than your vocab_size.\n",
    "        print(f\"vocab size: {model.config.vocab_size}\")\n",
    "        print(f\"batch min ID: {input_ids.min()}, max ID: {input_ids.max()}\")\n",
    "        # --- END DEBUGGING STEP ---\n",
    "\n",
    "        print(\"performing forward pass...\")\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        print(\"calculating loss...\")\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(\"performing backward pass...\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"epoch {epoch} complete, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0529f-51e3-4a64-8640-7a69c3487ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
